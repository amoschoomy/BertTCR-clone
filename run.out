
Starting BertTCR prediction script...

Arguments:
sample_dir: /scratch/project/tcr_ml/BertTCR/seekgene_embedding
model_file: /scratch/project/tcr_ml/BertTCR/TrainedModels/Pretrained_multiple_cancer_detection.pth
tcr_num: 100
max_length: 24
kernel_size: [2, 3, 4]
filter_num: [3, 2, 1]
dropout: 0.4
device: cuda:0
output: ./ZERO_prediction.tsv

Loading model...
Initializing BertTCR with parameters:
filter_num: [3, 2, 1]
kernel_size: [2, 3, 4]
ins_num: 100
drop_out: 0.4
Model loaded successfully

Starting prediction...

Processing sample: S2.tsv_batch_7_embedding.pt
Loaded sample shape: torch.Size([100, 22, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 22, 768])
Input matrix size: 1689600

Making prediction...
Original input shape: torch.Size([100, 22, 768])
Shape after padding: torch.Size([100, 24, 768])
Shape after permute: torch.Size([100, 768, 24])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.6770586915339731
Prediction: True

Processing sample: S1.tsv_batch_19_embedding.pt
Loaded sample shape: torch.Size([100, 20, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 20, 768])
Input matrix size: 1536000

Making prediction...
Original input shape: torch.Size([100, 20, 768])
Shape after padding: torch.Size([100, 24, 768])
Shape after permute: torch.Size([100, 768, 24])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.6852400756093204
Prediction: True

Processing sample: S1.tsv_batch_23_embedding.pt
Loaded sample shape: torch.Size([100, 23, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 23, 768])
Input matrix size: 1766400

Making prediction...
Original input shape: torch.Size([100, 23, 768])
Shape after padding: torch.Size([100, 24, 768])
Shape after permute: torch.Size([100, 768, 24])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.656265118255815
Prediction: True

Processing sample: S1.tsv_batch_18_embedding.pt
Loaded sample shape: torch.Size([100, 22, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 22, 768])
Input matrix size: 1689600

Making prediction...
Original input shape: torch.Size([100, 22, 768])
Shape after padding: torch.Size([100, 24, 768])
Shape after permute: torch.Size([100, 768, 24])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.5866757029096359
Prediction: True

Processing sample: S1.tsv_batch_5_embedding.pt
Loaded sample shape: torch.Size([100, 21, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 21, 768])
Input matrix size: 1612800

Making prediction...
Original input shape: torch.Size([100, 21, 768])
Shape after padding: torch.Size([100, 24, 768])
Shape after permute: torch.Size([100, 768, 24])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.6435165605040665
Prediction: True

Processing sample: S1.tsv_batch_10_embedding.pt
Loaded sample shape: torch.Size([100, 23, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 23, 768])
Input matrix size: 1766400

Making prediction...
Original input shape: torch.Size([100, 23, 768])
Shape after padding: torch.Size([100, 24, 768])
Shape after permute: torch.Size([100, 768, 24])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.7136819443183284
Prediction: True

Processing sample: S3.tsv_batch_12_embedding.pt
Loaded sample shape: torch.Size([100, 20, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 20, 768])
Input matrix size: 1536000

Making prediction...
Original input shape: torch.Size([100, 20, 768])
Shape after padding: torch.Size([100, 24, 768])
Shape after permute: torch.Size([100, 768, 24])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.4479106641196029
Prediction: False

Processing sample: S1.tsv_batch_26_embedding.pt
Loaded sample shape: torch.Size([100, 20, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 20, 768])
Input matrix size: 1536000

Making prediction...
Original input shape: torch.Size([100, 20, 768])
Shape after padding: torch.Size([100, 24, 768])
Shape after permute: torch.Size([100, 768, 24])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.5707814020501145
Prediction: True

Processing sample: S3.tsv_batch_16_embedding.pt
Loaded sample shape: torch.Size([13, 18, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([13, 18, 768])
Input matrix size: 179712

Making prediction...
Original input shape: torch.Size([13, 18, 768])
Shape after padding: torch.Size([13, 24, 768])
Shape after permute: torch.Size([13, 768, 24])
Shapes after individual convolutions: [torch.Size([13, 3, 1]), torch.Size([13, 2, 1]), torch.Size([13, 1, 1])]
Shape after concatenation: torch.Size([13, 6, 1])
Shape after first reshape: torch.Size([13, 1, 6])
Shape after FC layer and dropout: torch.Size([13, 1, 1])
Total elements before problematic reshape: 13
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.49137208580362646
Prediction: False

Processing sample: S2.tsv_batch_1_embedding.pt
Loaded sample shape: torch.Size([100, 22, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 22, 768])
Input matrix size: 1689600

Making prediction...
Original input shape: torch.Size([100, 22, 768])
Shape after padding: torch.Size([100, 24, 768])
Shape after permute: torch.Size([100, 768, 24])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.628463121850948
Prediction: True

Processing sample: S1.tsv_batch_20_embedding.pt
Loaded sample shape: torch.Size([100, 23, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 23, 768])
Input matrix size: 1766400

Making prediction...
Original input shape: torch.Size([100, 23, 768])
Shape after padding: torch.Size([100, 24, 768])
Shape after permute: torch.Size([100, 768, 24])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.5954124596218598
Prediction: True

Processing sample: S3.tsv_batch_6_embedding.pt
Loaded sample shape: torch.Size([100, 21, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 21, 768])
Input matrix size: 1612800

Making prediction...
Original input shape: torch.Size([100, 21, 768])
Shape after padding: torch.Size([100, 24, 768])
Shape after permute: torch.Size([100, 768, 24])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.4824570186138774
Prediction: False

Processing sample: S3.tsv_batch_3_embedding.pt
Loaded sample shape: torch.Size([100, 21, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 21, 768])
Input matrix size: 1612800

Making prediction...
Original input shape: torch.Size([100, 21, 768])
Shape after padding: torch.Size([100, 24, 768])
Shape after permute: torch.Size([100, 768, 24])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.46248758281662516
Prediction: False

Processing sample: S2.tsv_batch_3_embedding.pt
Loaded sample shape: torch.Size([100, 23, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 23, 768])
Input matrix size: 1766400

Making prediction...
Original input shape: torch.Size([100, 23, 768])
Shape after padding: torch.Size([100, 24, 768])
Shape after permute: torch.Size([100, 768, 24])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.7139563331405407
Prediction: True

Processing sample: S1.tsv_batch_11_embedding.pt
Loaded sample shape: torch.Size([100, 23, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 23, 768])
Input matrix size: 1766400

Making prediction...
Original input shape: torch.Size([100, 23, 768])
Shape after padding: torch.Size([100, 24, 768])
Shape after permute: torch.Size([100, 768, 24])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.6794834406255386
Prediction: True

Processing sample: S3.tsv_batch_11_embedding.pt
Loaded sample shape: torch.Size([100, 21, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 21, 768])
Input matrix size: 1612800

Making prediction...
Original input shape: torch.Size([100, 21, 768])
Shape after padding: torch.Size([100, 24, 768])
Shape after permute: torch.Size([100, 768, 24])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.6024411188810014
Prediction: True

Processing sample: S1.tsv_batch_17_embedding.pt
Loaded sample shape: torch.Size([100, 21, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 21, 768])
Input matrix size: 1612800

Making prediction...
Original input shape: torch.Size([100, 21, 768])
Shape after padding: torch.Size([100, 24, 768])
Shape after permute: torch.Size([100, 768, 24])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.32472849608410326
Prediction: False

Processing sample: S1.tsv_batch_1_embedding.pt
Loaded sample shape: torch.Size([100, 21, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 21, 768])
Input matrix size: 1612800

Making prediction...
Original input shape: torch.Size([100, 21, 768])
Shape after padding: torch.Size([100, 24, 768])
Shape after permute: torch.Size([100, 768, 24])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.6702180600055404
Prediction: True

Processing sample: S3.tsv_batch_2_embedding.pt
Loaded sample shape: torch.Size([100, 21, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 21, 768])
Input matrix size: 1612800

Making prediction...
Original input shape: torch.Size([100, 21, 768])
Shape after padding: torch.Size([100, 24, 768])
Shape after permute: torch.Size([100, 768, 24])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.6139003943688374
Prediction: True

Processing sample: S1.tsv_batch_25_embedding.pt
Loaded sample shape: torch.Size([100, 21, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 21, 768])
Input matrix size: 1612800

Making prediction...
Original input shape: torch.Size([100, 21, 768])
Shape after padding: torch.Size([100, 24, 768])
Shape after permute: torch.Size([100, 768, 24])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.6726507817804486
Prediction: True

Processing sample: S1.tsv_batch_6_embedding.pt
Loaded sample shape: torch.Size([100, 23, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 23, 768])
Input matrix size: 1766400

Making prediction...
Original input shape: torch.Size([100, 23, 768])
Shape after padding: torch.Size([100, 24, 768])
Shape after permute: torch.Size([100, 768, 24])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.6312801438967293
Prediction: True

Processing sample: S2.tsv_batch_9_embedding.pt
Loaded sample shape: torch.Size([100, 21, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 21, 768])
Input matrix size: 1612800

Making prediction...
Original input shape: torch.Size([100, 21, 768])
Shape after padding: torch.Size([100, 24, 768])
Shape after permute: torch.Size([100, 768, 24])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.6624087822195489
Prediction: True

Processing sample: S1.tsv_batch_22_embedding.pt
Loaded sample shape: torch.Size([100, 21, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 21, 768])
Input matrix size: 1612800

Making prediction...
Original input shape: torch.Size([100, 21, 768])
Shape after padding: torch.Size([100, 24, 768])
Shape after permute: torch.Size([100, 768, 24])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.491522215763386
Prediction: False

Processing sample: S1.tsv_batch_27_embedding.pt
Loaded sample shape: torch.Size([100, 23, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 23, 768])
Input matrix size: 1766400

Making prediction...
Original input shape: torch.Size([100, 23, 768])
Shape after padding: torch.Size([100, 24, 768])
Shape after permute: torch.Size([100, 768, 24])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.6434205395320642
Prediction: True

Processing sample: S1.tsv_batch_24_embedding.pt
Loaded sample shape: torch.Size([100, 21, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 21, 768])
Input matrix size: 1612800

Making prediction...
Original input shape: torch.Size([100, 21, 768])
Shape after padding: torch.Size([100, 24, 768])
Shape after permute: torch.Size([100, 768, 24])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.6419274697595321
Prediction: True

Processing sample: S1.tsv_batch_21_embedding.pt
Loaded sample shape: torch.Size([100, 20, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 20, 768])
Input matrix size: 1536000

Making prediction...
Original input shape: torch.Size([100, 20, 768])
Shape after padding: torch.Size([100, 24, 768])
Shape after permute: torch.Size([100, 768, 24])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.6106942648745627
Prediction: True

Processing sample: S3.tsv_batch_15_embedding.pt
Loaded sample shape: torch.Size([100, 23, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 23, 768])
Input matrix size: 1766400

Making prediction...
Original input shape: torch.Size([100, 23, 768])
Shape after padding: torch.Size([100, 24, 768])
Shape after permute: torch.Size([100, 768, 24])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.4685080907401803
Prediction: False

Processing sample: S1.tsv_batch_15_embedding.pt
Loaded sample shape: torch.Size([100, 20, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 20, 768])
Input matrix size: 1536000

Making prediction...
Original input shape: torch.Size([100, 20, 768])
Shape after padding: torch.Size([100, 24, 768])
Shape after permute: torch.Size([100, 768, 24])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.649206858389797
Prediction: True

Processing sample: S1.tsv_batch_14_embedding.pt
Loaded sample shape: torch.Size([100, 21, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 21, 768])
Input matrix size: 1612800

Making prediction...
Original input shape: torch.Size([100, 21, 768])
Shape after padding: torch.Size([100, 24, 768])
Shape after permute: torch.Size([100, 768, 24])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.6429028371076925
Prediction: True

Processing sample: S1.tsv_batch_3_embedding.pt
Loaded sample shape: torch.Size([100, 21, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 21, 768])
Input matrix size: 1612800

Making prediction...
Original input shape: torch.Size([100, 21, 768])
Shape after padding: torch.Size([100, 24, 768])
Shape after permute: torch.Size([100, 768, 24])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.5125844345108812
Prediction: True

Processing sample: S1.tsv_batch_9_embedding.pt
Loaded sample shape: torch.Size([100, 22, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 22, 768])
Input matrix size: 1689600

Making prediction...
Original input shape: torch.Size([100, 22, 768])
Shape after padding: torch.Size([100, 24, 768])
Shape after permute: torch.Size([100, 768, 24])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.7071757940990027
Prediction: True

Processing sample: S3.tsv_batch_8_embedding.pt
Loaded sample shape: torch.Size([100, 24, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 24, 768])
Input matrix size: 1843200

Making prediction...
Original input shape: torch.Size([100, 24, 768])
Shape after permute: torch.Size([100, 768, 24])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.6975355073501978
Prediction: True

Processing sample: S2.tsv_batch_12_embedding.pt
Loaded sample shape: torch.Size([53, 20, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([53, 20, 768])
Input matrix size: 814080

Making prediction...
Original input shape: torch.Size([53, 20, 768])
Shape after padding: torch.Size([53, 24, 768])
Shape after permute: torch.Size([53, 768, 24])
Shapes after individual convolutions: [torch.Size([53, 3, 1]), torch.Size([53, 2, 1]), torch.Size([53, 1, 1])]
Shape after concatenation: torch.Size([53, 6, 1])
Shape after first reshape: torch.Size([53, 1, 6])
Shape after FC layer and dropout: torch.Size([53, 1, 1])
Total elements before problematic reshape: 53
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.6112499071823403
Prediction: True

Processing sample: S1.tsv_batch_4_embedding.pt
Loaded sample shape: torch.Size([100, 22, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 22, 768])
Input matrix size: 1689600

Making prediction...
Original input shape: torch.Size([100, 22, 768])
Shape after padding: torch.Size([100, 24, 768])
Shape after permute: torch.Size([100, 768, 24])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.5405860351063856
Prediction: True

Processing sample: S3.tsv_batch_4_embedding.pt
Loaded sample shape: torch.Size([100, 20, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 20, 768])
Input matrix size: 1536000

Making prediction...
Original input shape: torch.Size([100, 20, 768])
Shape after padding: torch.Size([100, 24, 768])
Shape after permute: torch.Size([100, 768, 24])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.7008967715588237
Prediction: True

Processing sample: S2.tsv_batch_8_embedding.pt
Loaded sample shape: torch.Size([100, 20, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 20, 768])
Input matrix size: 1536000

Making prediction...
Original input shape: torch.Size([100, 20, 768])
Shape after padding: torch.Size([100, 24, 768])
Shape after permute: torch.Size([100, 768, 24])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.6598401557676313
Prediction: True

Processing sample: S1.tsv_batch_16_embedding.pt
Loaded sample shape: torch.Size([100, 23, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 23, 768])
Input matrix size: 1766400

Making prediction...
Original input shape: torch.Size([100, 23, 768])
Shape after padding: torch.Size([100, 24, 768])
Shape after permute: torch.Size([100, 768, 24])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.6424602855230999
Prediction: True

Processing sample: S3.tsv_batch_13_embedding.pt
Loaded sample shape: torch.Size([100, 22, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 22, 768])
Input matrix size: 1689600

Making prediction...
Original input shape: torch.Size([100, 22, 768])
Shape after padding: torch.Size([100, 24, 768])
Shape after permute: torch.Size([100, 768, 24])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.6617837958475623
Prediction: True

Processing sample: S2.tsv_batch_4_embedding.pt
Loaded sample shape: torch.Size([100, 21, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 21, 768])
Input matrix size: 1612800

Making prediction...
Original input shape: torch.Size([100, 21, 768])
Shape after padding: torch.Size([100, 24, 768])
Shape after permute: torch.Size([100, 768, 24])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.4502780877503447
Prediction: False

Processing sample: S3.tsv_batch_7_embedding.pt
Loaded sample shape: torch.Size([100, 21, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 21, 768])
Input matrix size: 1612800

Making prediction...
Original input shape: torch.Size([100, 21, 768])
Shape after padding: torch.Size([100, 24, 768])
Shape after permute: torch.Size([100, 768, 24])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.6817737723749766
Prediction: True

Processing sample: S2.tsv_batch_6_embedding.pt
Loaded sample shape: torch.Size([100, 20, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 20, 768])
Input matrix size: 1536000

Making prediction...
Original input shape: torch.Size([100, 20, 768])
Shape after padding: torch.Size([100, 24, 768])
Shape after permute: torch.Size([100, 768, 24])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.4992228305638083
Prediction: False

Processing sample: S3.tsv_batch_1_embedding.pt
Loaded sample shape: torch.Size([100, 23, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 23, 768])
Input matrix size: 1766400

Making prediction...
Original input shape: torch.Size([100, 23, 768])
Shape after padding: torch.Size([100, 24, 768])
Shape after permute: torch.Size([100, 768, 24])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.4329198010779239
Prediction: False

Processing sample: S3.tsv_batch_5_embedding.pt
Loaded sample shape: torch.Size([100, 21, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 21, 768])
Input matrix size: 1612800

Making prediction...
Original input shape: torch.Size([100, 21, 768])
Shape after padding: torch.Size([100, 24, 768])
Shape after permute: torch.Size([100, 768, 24])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.64740005499431
Prediction: True

Processing sample: S3.tsv_batch_14_embedding.pt
Loaded sample shape: torch.Size([100, 22, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 22, 768])
Input matrix size: 1689600

Making prediction...
Original input shape: torch.Size([100, 22, 768])
Shape after padding: torch.Size([100, 24, 768])
Shape after permute: torch.Size([100, 768, 24])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.5603200776587914
Prediction: True

Processing sample: S2.tsv_batch_11_embedding.pt
Loaded sample shape: torch.Size([100, 21, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 21, 768])
Input matrix size: 1612800

Making prediction...
Original input shape: torch.Size([100, 21, 768])
Shape after padding: torch.Size([100, 24, 768])
Shape after permute: torch.Size([100, 768, 24])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.5795113570699221
Prediction: True

Processing sample: S1.tsv_batch_28_embedding.pt
Loaded sample shape: torch.Size([100, 26, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 26, 768])
Input matrix size: 1996800

Making prediction...
Original input shape: torch.Size([100, 26, 768])
Shape after permute: torch.Size([100, 768, 26])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.7222377083580419
Prediction: True

Processing sample: S1.tsv_batch_12_embedding.pt
Loaded sample shape: torch.Size([100, 22, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 22, 768])
Input matrix size: 1689600

Making prediction...
Original input shape: torch.Size([100, 22, 768])
Shape after padding: torch.Size([100, 24, 768])
Shape after permute: torch.Size([100, 768, 24])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.4201519695237378
Prediction: False

Processing sample: S1.tsv_batch_7_embedding.pt
Loaded sample shape: torch.Size([100, 26, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 26, 768])
Input matrix size: 1996800

Making prediction...
Original input shape: torch.Size([100, 26, 768])
Shape after permute: torch.Size([100, 768, 26])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.49415275290501753
Prediction: False

Processing sample: S2.tsv_batch_2_embedding.pt
Loaded sample shape: torch.Size([100, 20, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 20, 768])
Input matrix size: 1536000

Making prediction...
Original input shape: torch.Size([100, 20, 768])
Shape after padding: torch.Size([100, 24, 768])
Shape after permute: torch.Size([100, 768, 24])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.6252034789412478
Prediction: True

Processing sample: S3.tsv_batch_10_embedding.pt
Loaded sample shape: torch.Size([100, 23, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 23, 768])
Input matrix size: 1766400

Making prediction...
Original input shape: torch.Size([100, 23, 768])
Shape after padding: torch.Size([100, 24, 768])
Shape after permute: torch.Size([100, 768, 24])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.300332433598304
Prediction: False

Processing sample: S1.tsv_batch_29_embedding.pt
Loaded sample shape: torch.Size([100, 26, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 26, 768])
Input matrix size: 1996800

Making prediction...
Original input shape: torch.Size([100, 26, 768])
Shape after permute: torch.Size([100, 768, 26])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.6469459802502184
Prediction: True

Processing sample: S1.tsv_batch_8_embedding.pt
Loaded sample shape: torch.Size([100, 26, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 26, 768])
Input matrix size: 1996800

Making prediction...
Original input shape: torch.Size([100, 26, 768])
Shape after permute: torch.Size([100, 768, 26])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.39338501152387245
Prediction: False

Processing sample: S1.tsv_batch_13_embedding.pt
Loaded sample shape: torch.Size([100, 23, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 23, 768])
Input matrix size: 1766400

Making prediction...
Original input shape: torch.Size([100, 23, 768])
Shape after padding: torch.Size([100, 24, 768])
Shape after permute: torch.Size([100, 768, 24])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.5217522087448349
Prediction: True

Processing sample: S1.tsv_batch_30_embedding.pt
Loaded sample shape: torch.Size([68, 23, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([68, 23, 768])
Input matrix size: 1201152

Making prediction...
Original input shape: torch.Size([68, 23, 768])
Shape after padding: torch.Size([68, 24, 768])
Shape after permute: torch.Size([68, 768, 24])
Shapes after individual convolutions: [torch.Size([68, 3, 1]), torch.Size([68, 2, 1]), torch.Size([68, 1, 1])]
Shape after concatenation: torch.Size([68, 6, 1])
Shape after first reshape: torch.Size([68, 1, 6])
Shape after FC layer and dropout: torch.Size([68, 1, 1])
Total elements before problematic reshape: 68
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.5194163090127577
Prediction: True

Processing sample: S3.tsv_batch_9_embedding.pt
Loaded sample shape: torch.Size([100, 22, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 22, 768])
Input matrix size: 1689600

Making prediction...
Original input shape: torch.Size([100, 22, 768])
Shape after padding: torch.Size([100, 24, 768])
Shape after permute: torch.Size([100, 768, 24])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.7018816255613676
Prediction: True

Processing sample: S1.tsv_batch_2_embedding.pt
Loaded sample shape: torch.Size([100, 23, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 23, 768])
Input matrix size: 1766400

Making prediction...
Original input shape: torch.Size([100, 23, 768])
Shape after padding: torch.Size([100, 24, 768])
Shape after permute: torch.Size([100, 768, 24])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.7167228757742032
Prediction: True

Processing sample: S2.tsv_batch_5_embedding.pt
Loaded sample shape: torch.Size([100, 22, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 22, 768])
Input matrix size: 1689600

Making prediction...
Original input shape: torch.Size([100, 22, 768])
Shape after padding: torch.Size([100, 24, 768])
Shape after permute: torch.Size([100, 768, 24])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.6995687106500502
Prediction: True

Processing sample: S2.tsv_batch_10_embedding.pt
Loaded sample shape: torch.Size([100, 22, 768])
Sample dtype: torch.float32
Sample device: cpu
Sample moved to device: cuda:0
Input matrix shape: torch.Size([100, 22, 768])
Input matrix size: 1689600

Making prediction...
Original input shape: torch.Size([100, 22, 768])
Shape after padding: torch.Size([100, 24, 768])
Shape after permute: torch.Size([100, 768, 24])
Shapes after individual convolutions: [torch.Size([100, 3, 1]), torch.Size([100, 2, 1]), torch.Size([100, 1, 1])]
Shape after concatenation: torch.Size([100, 6, 1])
Shape after first reshape: torch.Size([100, 1, 6])
Shape after FC layer and dropout: torch.Size([100, 1, 1])
Total elements before problematic reshape: 100
Shape after final reshape: torch.Size([1, 100])
Shape after model 0: torch.Size([1, 2])
Shape after model 1: torch.Size([1, 2])
Shape after model 2: torch.Size([1, 2])
Shape after model 3: torch.Size([1, 2])
Shape after model 4: torch.Size([1, 2])
Final output shape: torch.Size([1, 2])
Prediction shape: torch.Size([1, 2])
Probability: 0.6946135664155922
Prediction: True

Prediction completed
The prediction results have been saved to: ./ZERO_prediction.tsv
